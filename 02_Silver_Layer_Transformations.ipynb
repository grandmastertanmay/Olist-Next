{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d54de26-fb3b-49a0-853f-6b42dc328922",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ü•à Phase 2: The Silver Layer (Cleaning & Transformation)\n",
    "**Project:** \"Olist-Next\" Hyper-Personalized Retention Engine\n",
    "**Layer:** Silver (Enterprise/Validated Data)\n",
    "\n",
    "## üéØ Objectives\n",
    "We transform the \"Bronze\" (Raw) data into \"Silver\" (Clean) tables by applying specific business rules:\n",
    "1.  [cite_start]**Orders:** Filter for `order_status = 'delivered'` to focus on completed transactions  [cite_start]and cast timestamps.\n",
    "2.  [cite_start]**Reviews:** Handle null values in text fields (filling with \"No review text\")[cite: 71].\n",
    "3.  [cite_start]**Products:** Join with `category_translation` to provide English category names.\n",
    "4.  **Passthrough:** Clean and standardize the remaining tables (Items, Customers, Payments) for the Gold layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84eaadc5-60b4-49a4-9773-d7f5710726de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Setup (Python)\n",
    "Set the catalog context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1a8ecdf-e369-4f66-bcab-44c35a9f825f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_timestamp, lit\n",
    "\n",
    "# Set the Catalog Context\n",
    "spark.sql(\"USE CATALOG olist_hackathon\")\n",
    "\n",
    "print(\"‚úÖ Context set to 'olist_hackathon'. Ready for Silver transformations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2acfe88-df59-4634-a541-3583d06940a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Transformation 1 - Orders (Python)\n",
    "Logic: Filter for 'delivered' and fix timestamp types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fd02998-ef69-4c6e-9744-d95c0c2b47a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_silver_orders():\n",
    "    print(\"‚è≥ Processing Silver Orders...\")\n",
    "    \n",
    "    # 1. Read Bronze\n",
    "    df_orders = spark.table(\"bronze.orders\")\n",
    "    \n",
    "    # 2. Apply Transformations\n",
    "    df_cleaned = (df_orders\n",
    "                  # Filter: Only completed orders are relevant for Churn/CLV \n",
    "                  .filter(col(\"order_status\") == \"delivered\")\n",
    "                  # Fix Type: Convert string timestamp to Spark TimestampType \n",
    "                  .withColumn(\"order_purchase_timestamp\", \n",
    "                              to_timestamp(col(\"order_purchase_timestamp\")))\n",
    "                  .withColumn(\"order_delivered_customer_date\", \n",
    "                              to_timestamp(col(\"order_delivered_customer_date\")))\n",
    "                  .withColumn(\"order_estimated_delivery_date\", \n",
    "                              to_timestamp(col(\"order_estimated_delivery_date\")))\n",
    "                 )\n",
    "    \n",
    "    # 3. Write to Silver (Overwrite mode ensures idempotency)\n",
    "    df_cleaned.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"silver.orders\")\n",
    "    print(f\"‚úÖ silver.orders created. Count: {df_cleaned.count()}\")\n",
    "\n",
    "process_silver_orders()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9805842a-12a0-4349-9807-80fcfd21b516",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Transformation 2 - Reviews (Python)\n",
    "Logic: Handle nulls in the text column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a780ed2-67d7-41c5-94f7-8a8d3606b204",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_silver_reviews():\n",
    "    print(\"‚è≥ Processing Silver Reviews...\")\n",
    "    \n",
    "    df_reviews = spark.table(\"bronze.reviews\")\n",
    "    \n",
    "    # Logic: Replace null messages with a placeholder text [cite: 71]\n",
    "    # We use .fillna() specifically on the text column\n",
    "    df_cleaned = df_reviews.fillna({\"review_comment_message\": \"No review text\", \n",
    "                                    \"review_comment_title\": \"No Title\"})\n",
    "    \n",
    "    df_cleaned.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"silver.reviews\")\n",
    "    print(f\"‚úÖ silver.reviews created.\")\n",
    "\n",
    "process_silver_reviews()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88bc0905-7565-44d9-9c83-eb1499757242",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Transformation 3 - Products & Enrichment (Python)\n",
    "Logic: Join Products with Category Translations to get English names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc298d63-3e3e-4976-9d30-ef31ee2c00f0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 9"
    }
   },
   "outputs": [],
   "source": [
    "def process_silver_products():\n",
    "    print(\"‚è≥ Processing Silver Products (Enrichment)...\")\n",
    "    \n",
    "    # Read both tables\n",
    "    df_products = spark.table(\"bronze.products\")\n",
    "    df_trans = spark.table(\"bronze.category_translation\").drop(\"ingestion_ts\").drop(\"source_file\")\n",
    "    \n",
    "    # Logic: Join to get English names \n",
    "    # We use a Left Join to ensure we don't lose products if a translation is missing\n",
    "    df_joined = (df_products\n",
    "                 .join(df_trans, \"product_category_name\", \"left\")\n",
    "                 .drop(\"product_category_name\") # Drop original Portuguese column\n",
    "                 .withColumnRenamed(\"product_category_name_english\", \"category_name\") # Rename to clean standard\n",
    "                )\n",
    "    \n",
    "    # Drop _rescued_data column if it exists\n",
    "    if \"_rescued_data\" in df_joined.columns:\n",
    "        df_joined = df_joined.drop(\"_rescued_data\")\n",
    "    \n",
    "    df_joined.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"silver.products\")\n",
    "    print(f\"‚úÖ silver.products created with English names.\")\n",
    "\n",
    "process_silver_products()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb5b05a9-e3a8-4c43-8e9c-afe04e2d22c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### The \"Passthrough\" Tables (Python)\n",
    "You need these tables for the Gold Layer (churn calculation), so we move them to Silver even without complex logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9dc0b1ab-7011-4b6d-8ba0-e303e3508d4d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 11"
    }
   },
   "outputs": [],
   "source": [
    "def process_passthrough_tables():\n",
    "    # List of tables to move from Bronze to Silver as-is\n",
    "    tables = [\"order_items\", \"customers\", \"payments\", \"sellers\", \"geolocation\"]\n",
    "    \n",
    "    for table in tables:\n",
    "        # Robust Fix: Use the Fully Qualified Name (Catalog.Schema.Table)\n",
    "        source_path = f\"olist_hackathon.bronze.{table}\"\n",
    "        target_path = f\"olist_hackathon.silver.{table}\"\n",
    "        \n",
    "        print(f\"‚è≥ Passthrough processing: {source_path} -> {target_path}...\")\n",
    "        \n",
    "        try:\n",
    "            # Read from specific source\n",
    "            df = spark.table(source_path)\n",
    "            \n",
    "            # Write to specific target\n",
    "            df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(target_path)\n",
    "            print(f\"‚úÖ Created: {target_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error processing {table}: {e}\")\n",
    "            print(\"üí° Tip: Run 'SHOW TABLES IN olist_hackathon.bronze' to check if this table exists.\")\n",
    "\n",
    "process_passthrough_tables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "acbc617c-a606-4525-a21c-e746a7a16c62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Verification (Python)\n",
    "Proof of success: Check the English category names in the products table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f301588-0137-49c6-8821-4ab2e77f249c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 13"
    }
   },
   "outputs": [],
   "source": [
    "# Verify the Join\n",
    "print(\"--- Silver Products Sample (English Categories) ---\")\n",
    "display(spark.table(\"olist_hackathon.silver.products\").select(\"product_id\", \"category_name\").limit(5))\n",
    "\n",
    "# Verify the Orders Filter\n",
    "print(\"--- Silver Orders Status Check (Should only be 'delivered') ---\")\n",
    "display(spark.sql(\"SELECT DISTINCT order_status FROM olist_hackathon.silver.orders\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75e3c458-d6d1-4d0c-9c0b-f63c8fb38a00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_Silver_Layer_Transformations",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
