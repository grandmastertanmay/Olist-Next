{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "185d081f-30dc-4df8-9832-e88e5b7aa1fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ðŸš€ Phase 1: Environment Setup & Bronze Ingestion\n",
    "**Project:** \"Olist-Next\" Hyper-Personalized Retention Engine\n",
    "**Architecture:** Medallion Lakehouse (Bronze -> Silver -> Gold)\n",
    "**Author:** Tanmay Patil\n",
    "\n",
    "## ðŸ“‹ Overview\n",
    "This notebook establishes the foundation of the Lakehouse. [cite_start]We implement the **Medallion Architecture** to guarantee data quality and lineage[cite: 22].\n",
    "1.  **Environment:** Setup Unity Catalog, Schemas, and Volumes.\n",
    "2.  **Ingestion:** We use **Auto Loader** (`cloudFiles`) to ingest raw CSVs.\n",
    "    * *Why Auto Loader?* It provides schema inference and robustness. [cite_start]Any malformed records (e.g., strings in integer columns) are captured in a special `_rescued_data` column rather than failing the pipeline[cite: 55, 56].\n",
    "    * [cite_start]*Schema Evolution:* It automatically adapts if the source CSV schema changes[cite: 54]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d59ea83b-d1fc-4900-9138-8e18afd85561",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read the parameter passed from the Job\n",
    "dbutils.widgets.text(\"env\", \"dev\") # Default to 'dev' if run manually\n",
    "current_env = dbutils.widgets.get(\"env\")\n",
    "\n",
    "print(f\"ðŸš€ Running in Environment: {current_env.upper()}\")\n",
    "\n",
    "# Logic: Only Drop/Recreate tables if in Dev; in Prod, just append.\n",
    "if current_env == \"dev\":\n",
    "    print(\"âš ï¸ DEV MODE: Resetting Schema...\")\n",
    "    spark.sql(\"DROP SCHEMA IF EXISTS olist_hackathon.bronze CASCADE\")\n",
    "    spark.sql(\"CREATE SCHEMA IF NOT EXISTS olist_hackathon.bronze\")\n",
    "else:\n",
    "    print(\"âœ… PROD MODE: Appending to existing Schema.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d31f8de-041a-403e-ba9a-1822d4de0be7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Unity Catalog Setup (SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4108efe9-e088-4414-acf3-a0f66fb20ace",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- 1. Create the Project Catalog\n",
    "-- This serves as the top-level container for all our assets\n",
    "CREATE CATALOG IF NOT EXISTS olist_hackathon;\n",
    "\n",
    "-- 2. Set the active context\n",
    "USE CATALOG olist_hackathon;\n",
    "\n",
    "-- 3. Create the Medallion Layers [cite: 29]\n",
    "-- Bronze: Raw ingestion (as-is)\n",
    "CREATE SCHEMA IF NOT EXISTS bronze;\n",
    "-- Silver: Cleaned, filtered, and enriched\n",
    "CREATE SCHEMA IF NOT EXISTS silver;\n",
    "-- Gold: Business-level aggregates and ML features\n",
    "CREATE SCHEMA IF NOT EXISTS gold;\n",
    "\n",
    "-- 4. Verify creation\n",
    "DESCRIBE DATABASE EXTENDED bronze;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e26e63a-396e-4857-bd68-e67ad8442056",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Ingestion Configuration (Python)\n",
    "We define our file paths and the mapping dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71e4eb09-dd92-4613-80b9-0f1da74c934f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- CONFIGURATION ---\n",
    "# The path where we uploaded the Kaggle CSVs\n",
    "SOURCE_PATH = \"/Volumes/olist_hackathon/default/raw_data/\"\n",
    "\n",
    "# Auto Loader requires a checkpoint location to track which files it has already processed\n",
    "CHECKPOINT_BASE = \"/Volumes/olist_hackathon/default/checkpoints/\"\n",
    "\n",
    "# Map raw filenames to clean, friendly Bronze table names \n",
    "# We map 'olist_orders_dataset.csv' -> 'orders', etc.\n",
    "file_mapping = {\n",
    "    \"olist_orders_dataset.csv\": \"orders\",\n",
    "    \"olist_order_items_dataset.csv\": \"order_items\",\n",
    "    \"olist_order_reviews_dataset.csv\": \"reviews\",\n",
    "    \"olist_products_dataset.csv\": \"products\",\n",
    "    \"olist_customers_dataset.csv\": \"customers\",\n",
    "    \"olist_sellers_dataset.csv\": \"sellers\",\n",
    "    \"olist_order_payments_dataset.csv\": \"payments\",\n",
    "    \"olist_geolocation_dataset.csv\": \"geolocation\",\n",
    "    \"product_category_name_translation.csv\": \"category_translation\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b0f5624-06e6-42b0-b21f-4dbc708a4cd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### The Auto Loader Function (Python)\n",
    "This is the core engine. It uses spark.readStream with trigger(availableNow=True) to process files efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a6f14ff-099f-4c19-8bbb-469cba634e1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, input_file_name\n",
    "\n",
    "def ingest_to_bronze(source_file, table_name):\n",
    "    \"\"\"\n",
    "    Ingests raw CSV files into a Delta Table using Databricks Auto Loader.\n",
    "    \n",
    "    Args:\n",
    "        source_file (str): The specific CSV filename (e.g., 'olist_orders_dataset.csv')\n",
    "        table_name (str): The target table name (e.g., 'orders')\n",
    "    \"\"\"\n",
    "    source_full_path = f\"{SOURCE_PATH}{source_file}\"\n",
    "    checkpoint_path = f\"{CHECKPOINT_BASE}{table_name}\"\n",
    "    target_table = f\"olist_hackathon.bronze.{table_name}\"\n",
    "    \n",
    "    print(f\"â³ Starting ingestion: {source_file} -> {target_table}...\")\n",
    "    \n",
    "    # --- AUTO LOADER DEFINITION ---\n",
    "    # We use .format(\"cloudFiles\") for Auto Loader \n",
    "    df_stream = (spark.readStream\n",
    "                 .format(\"cloudFiles\")\n",
    "                 .option(\"cloudFiles.format\", \"csv\")\n",
    "                 # Critical: Infer types (Int, Double) instead of defaulting to Strings [cite: 60]\n",
    "                 .option(\"cloudFiles.inferColumnTypes\", \"true\") \n",
    "                 .option(\"header\", \"true\")\n",
    "                 # Critical: Location to store inferred schema for evolution [cite: 54]\n",
    "                 .option(\"cloudFiles.schemaLocation\", checkpoint_path)\n",
    "                 .load(source_full_path))\n",
    "    \n",
    "    # --- METADATA ENRICHMENT ---\n",
    "    # Adding timestamp and source filename helps with debugging and lineage\n",
    "    df_final = df_stream.withColumn(\"ingestion_ts\", current_timestamp()) \\\n",
    "                        .withColumn(\"source_file\", input_file_name())\n",
    "    \n",
    "    # --- WRITE TO DELTA ---\n",
    "    # trigger(availableNow=True) processes all available data then stops (Cost efficient)\n",
    "    query = (df_final.writeStream\n",
    "             .format(\"delta\")\n",
    "             .outputMode(\"append\")\n",
    "             .option(\"checkpointLocation\", checkpoint_path)\n",
    "             .option(\"mergeSchema\", \"true\") # Enable schema evolution [cite: 54]\n",
    "             .trigger(availableNow=True)\n",
    "             .table(target_table))\n",
    "    \n",
    "    query.awaitTermination()\n",
    "    print(f\"âœ… Ingestion Complete: {table_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "043b6b12-aafc-48fe-8931-9a2ce9e7ac5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Execution Loop (Python)\n",
    "Run the function for all 9 files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec8b3b16-22ab-4537-8a69-be2ff8ac87ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, col\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "SOURCE_PATH = \"/Volumes/olist_hackathon/default/raw_data/\"\n",
    "CHECKPOINT_BASE = \"/Volumes/olist_hackathon/default/checkpoints/\"\n",
    "\n",
    "file_mapping = {\n",
    "    \"olist_orders_dataset.csv\": \"orders\",\n",
    "    \"olist_order_items_dataset.csv\": \"order_items\",\n",
    "    \"olist_order_reviews_dataset.csv\": \"reviews\",\n",
    "    \"olist_products_dataset.csv\": \"products\",\n",
    "    \"olist_customers_dataset.csv\": \"customers\",\n",
    "    \"olist_sellers_dataset.csv\": \"sellers\",\n",
    "    \"olist_order_payments_dataset.csv\": \"payments\",\n",
    "    \"olist_geolocation_dataset.csv\": \"geolocation\",\n",
    "    \"product_category_name_translation.csv\": \"category_translation\"\n",
    "}\n",
    "\n",
    "# --- CORRECTED FUNCTION ---\n",
    "def ingest_to_bronze(file_name, table_name):\n",
    "    \"\"\"\n",
    "    Ingests specific CSV files into Delta Tables using Auto Loader.\n",
    "    Fix: Uses _metadata.file_path instead of input_file_name() for Unity Catalog compliance.\n",
    "    \"\"\"\n",
    "    checkpoint_path = f\"{CHECKPOINT_BASE}{table_name}\"\n",
    "    target_table = f\"olist_hackathon.bronze.{table_name}\"\n",
    "    \n",
    "    print(f\"â³ Starting ingestion: {file_name} -> {target_table}...\")\n",
    "    \n",
    "    # 1. Read Stream\n",
    "    df_stream = (spark.readStream\n",
    "                 .format(\"cloudFiles\")\n",
    "                 .option(\"cloudFiles.format\", \"csv\")\n",
    "                 .option(\"cloudFiles.inferColumnTypes\", \"true\") \n",
    "                 .option(\"header\", \"true\")\n",
    "                 .option(\"cloudFiles.schemaLocation\", checkpoint_path)\n",
    "                 .option(\"pathGlobFilter\", file_name) # Filter for specific file\n",
    "                 .load(SOURCE_PATH)) # Load from Parent Directory\n",
    "    \n",
    "    # 2. Metadata Enrichment (THE FIX)\n",
    "    # We access the hidden '_metadata' struct to get the file path\n",
    "    df_final = df_stream.withColumn(\"ingestion_ts\", current_timestamp()) \\\n",
    "                        .withColumn(\"source_file\", col(\"_metadata.file_path\"))\n",
    "    \n",
    "    # 3. Write to Delta\n",
    "    query = (df_final.writeStream\n",
    "             .format(\"delta\")\n",
    "             .outputMode(\"append\")\n",
    "             .option(\"checkpointLocation\", checkpoint_path)\n",
    "             .option(\"mergeSchema\", \"true\")\n",
    "             .trigger(availableNow=True)\n",
    "             .table(target_table))\n",
    "    \n",
    "    query.awaitTermination()\n",
    "    print(f\"âœ… Ingestion Complete: {table_name}\")\n",
    "\n",
    "# --- EXECUTION ---\n",
    "for file_name, table_name in file_mapping.items():\n",
    "    try:\n",
    "        ingest_to_bronze(file_name, table_name)\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error ingesting {table_name}: {str(e)}\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ All Jobs Finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8b75099-069e-44d3-8100-24826183ba47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check one table to ensure the source_file column is populated\n",
    "display(spark.sql(\"SELECT * FROM olist_hackathon.bronze.orders LIMIT 5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02d3c02c-2a97-4442-8147-538d57fa1779",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Visualize the table list\n",
    "print(\"Tables in Bronze Schema:\")\n",
    "display(spark.sql(\"SHOW TABLES IN olist_hackathon.bronze\"))\n",
    "\n",
    "# 2. Deep Dive: Check the 'Orders' table\n",
    "df_orders = spark.table(\"olist_hackathon.bronze.orders\")\n",
    "print(f\"Count of Orders: {df_orders.count()}\")\n",
    "\n",
    "# 3. Quality Check: Did Auto Loader rescue any malformed data? [cite: 55]\n",
    "# If this count is > 0, we have data quality issues in the source file.\n",
    "rescued_count = df_orders.filter(\"_rescued_data IS NOT NULL\").count()\n",
    "print(f\"Malformed Records Rescued: {rescued_count}\")\n",
    "\n",
    "display(df_orders.limit(5))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5089548376086955,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01_Setup_and_Bronze_Ingestion",
   "widgets": {
    "env": {
     "currentValue": "dev",
     "nuid": "fb438e72-30d3-4d28-aa24-921c99faae43",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "dev",
      "label": null,
      "name": "env",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "dev",
      "label": null,
      "name": "env",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
