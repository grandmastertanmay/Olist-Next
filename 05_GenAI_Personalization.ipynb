{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c23eea0-fe4c-4d05-bce6-936f4011f4f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ðŸª„ Phase 5: GenAI Personalization Engine\n",
    "**Project:** \"Olist-Next\" Retention Engine\n",
    "**Goal:** Generate hyper-personalized retention emails using Llama 3.\n",
    "\n",
    "## ðŸ§  Strategy (The \"Brain\")\n",
    "We don't just send generic spam. We use **Context-Aware AI**:\n",
    "1.  **Targeting:** High-Value Customers (`>$100`) who are At-Risk.\n",
    "2.  **Sentiment Adaptation:**\n",
    "    * **Low Score (< 3):** The AI will write an *apologetic* email asking for a second chance.\n",
    "    * **High Score (> 4):** The AI will write a *grateful* email asking for a referral.\n",
    "    * **Neutral:** The AI will focus on a \"We Miss You\" discount.\n",
    "3.  **Model:** `databricks-meta-llama-3-1-70b-instruct` via MLflow Deployments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b898fe3-0abd-4a48-b214-dabc039a00d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Target Audience Filtering \n",
    "Select the top 10 high-value, at-risk customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d9e1364-bae5-4740-a43b-ba133b8670fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, desc\n",
    "\n",
    "# Set Context\n",
    "spark.sql(\"USE CATALOG olist_hackathon\")\n",
    "\n",
    "# 1. Load Gold Data\n",
    "df_gold = spark.table(\"gold.customer_360\")\n",
    "\n",
    "# 2. Filter Logic\n",
    "# - At Risk (is_churn_risk = 1)\n",
    "# - High Value (monetary_value > 100)\n",
    "# - Limit to 10 for Demo speed\n",
    "df_targets = (\n",
    "    df_gold\n",
    "    .filter((col(\"is_churn_risk\") == 1) & (col(\"monetary_value\") > 100))\n",
    "    .orderBy(desc(\"monetary_value\"))\n",
    "    .limit(10)\n",
    "    .select(\n",
    "        \"customer_unique_id\", \n",
    "        \"favorite_category\", \n",
    "        \"monetary_value\", \n",
    "        \"avg_review_score\"\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"ðŸŽ¯ Target Audience Selected (Top 10 High-Value/At-Risk):\")\n",
    "display(df_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad264635-0468-4689-a8df-f8b9e397e15a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Advanced Prompt Engineering \n",
    "Here we define the logic to change the prompt based on customer satisfaction (Review Score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5eac1284-6f44-450f-953e-1e5915344a82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, concat, lit, when\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Define Prompt Logic as a UDF (User Defined Function)\n",
    "# This creates the specific instruction for Llama 3 based on the user's history\n",
    "def build_prompt_udf(category, spend, score):\n",
    "    \n",
    "    # 1. Determine Tone based on Review Score\n",
    "    if score <= 3:\n",
    "        tone = \"apologetic, humble, and concerned. We evidently failed them previously.\"\n",
    "        angle = \"Ask for a second chance to prove we have improved.\"\n",
    "    elif score >= 4.5:\n",
    "        tone = \"celebratory, warm, and grateful.\"\n",
    "        angle = \"Thank them for being a star customer. Ask if they would refer a friend.\"\n",
    "    else:\n",
    "        tone = \"friendly and inviting.\"\n",
    "        angle = \"Tell them we miss them.\"\n",
    "\n",
    "    # 2. Construct the Prompt\n",
    "    prompt = f\"\"\"\n",
    "    You are a Senior Customer Success Manager at Olist. Write a personalized retention email to a customer.\n",
    "    \n",
    "    Customer Profile:\n",
    "    - Favorite Category: {category}\n",
    "    - Total Spend: ${spend:.2f}\n",
    "    - Average Review Score: {score}/5.0\n",
    "    \n",
    "    Instructions:\n",
    "    - Tone: {tone}\n",
    "    - Strategy: {angle}\n",
    "    - Offer: Include a unique code 'OLIST_VIP' for 15% off.\n",
    "    - Constraint: Keep it under 100 words. Do not include a subject line.\n",
    "    \"\"\"\n",
    "    return prompt\n",
    "\n",
    "# Register UDF\n",
    "spark.udf.register(\"build_prompt\", build_prompt_udf, StringType())\n",
    "\n",
    "# Apply the Prompt Logic\n",
    "df_with_prompts = df_targets.withColumn(\n",
    "    \"llm_prompt\", \n",
    "    udf(build_prompt_udf, StringType())(col(\"favorite_category\"), col(\"monetary_value\"), col(\"avg_review_score\"))\n",
    ")\n",
    "\n",
    "# Verify the Personalization\n",
    "print(\"ðŸ“ Reviewing Generated Prompts (Notice the different Tones):\")\n",
    "display(df_with_prompts.select(\"avg_review_score\", \"llm_prompt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62fd3ba0-de84-4272-960d-38085bca529e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### The Generative Loop (Llama 3 Inference) \n",
    "We use pandas_udf to call the Databricks Foundation Model API efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f40b867-5518-425c-8a10-7cdab24455c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow.deployments\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf, col\n",
    "from typing import Iterator\n",
    "\n",
    "# Use the Llama 3.3 model you found\n",
    "ENDPOINT_NAME = \"databricks-meta-llama-3-3-70b-instruct\"\n",
    "\n",
    "@pandas_udf(\"string\")\n",
    "def generate_email_udf(prompts: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    \"\"\"\n",
    "    Calls the Databricks Foundation Model Endpoint (Sequential/Robust Mode).\n",
    "    Fixes 400 Error by sending strict OpenAI-format requests.\n",
    "    \"\"\"\n",
    "    # Initialize Client\n",
    "    client = mlflow.deployments.get_deploy_client(\"databricks\")\n",
    "    \n",
    "    for batch in prompts:\n",
    "        batch_output = []\n",
    "        \n",
    "        # Loop through each prompt in the batch individually\n",
    "        for prompt in batch:\n",
    "            # 1. Construct strict OpenAI-compatible payload\n",
    "            # The endpoint expects 'messages' at the root\n",
    "            payload = {\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                \"max_tokens\": 150,  # Optional: Limit length\n",
    "                \"temperature\": 0.7  # Optional: Creativity\n",
    "            }\n",
    "            \n",
    "            # 2. Call API\n",
    "            try:\n",
    "                # We pass the dictionary directly as 'inputs'\n",
    "                response = client.predict(endpoint=ENDPOINT_NAME, inputs=payload)\n",
    "                \n",
    "                # Extract text (Standard OpenAI format response)\n",
    "                text = response['choices'][0]['message']['content']\n",
    "                batch_output.append(text)\n",
    "                \n",
    "            except Exception as e:\n",
    "                # Error Handling\n",
    "                batch_output.append(f\"Error: {str(e)}\")\n",
    "        \n",
    "        yield pd.Series(batch_output)\n",
    "\n",
    "print(f\"â³ Calling GenAI Endpoint: {ENDPOINT_NAME}...\")\n",
    "\n",
    "# Run Inference\n",
    "df_final_campaign = df_with_prompts.withColumn(\"generated_email\", generate_email_udf(col(\"llm_prompt\")))\n",
    "\n",
    "# Show the Magic\n",
    "display(df_final_campaign.select(\"customer_unique_id\", \"avg_review_score\", \"generated_email\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdec1fdd-bb60-4b25-a009-f39b1de942bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow.deployments\n",
    "\n",
    "client = mlflow.deployments.get_deploy_client(\"databricks\")\n",
    "\n",
    "print(\"ðŸ” Scanning for available Foundation Models...\")\n",
    "try:\n",
    "    endpoints = client.list_endpoints()\n",
    "    found = False\n",
    "    for e in endpoints:\n",
    "        # Show all available endpoint IDs and names\n",
    "        print(f\"ID: {e.get('id', 'N/A')} | Name: {e.get('name', 'N/A')}\")\n",
    "        if e.get('name', '').startswith(\"databricks-\") and (\"instruct\" in e.get('name', '') or \"chat\" in e.get('name', '')):\n",
    "            print(f\"âœ… AVAILABLE: {e.get('name')}\")\n",
    "            found = True\n",
    "    \n",
    "    if not found:\n",
    "        print(\"âŒ No 'databricks-' foundation models found. You may need to enable them in the 'Serving' tab.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error listing endpoints: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1fff2d95-86ee-4d0a-a1b2-feddf78b0fab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Save to Gold \n",
    "Finalize the campaign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aca5ac6a-a4ae-496d-a4c7-c82cad638a9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Save the Campaign to the Gold Layer\n",
    "target_table = \"olist_hackathon.gold.retention_campaigns\"\n",
    "print(f\"â³ Saving final campaign to {target_table}...\")\n",
    "\n",
    "(df_final_campaign\n",
    " .select(\"customer_unique_id\", \"favorite_category\", \"monetary_value\", \"avg_review_score\", \"generated_email\")\n",
    " .write\n",
    " .format(\"delta\")\n",
    " .mode(\"overwrite\")\n",
    " .saveAsTable(target_table)\n",
    ")\n",
    "\n",
    "print(\"âœ… Campaign Data Saved!\")\n",
    "\n",
    "# 2. The \"Victory Lap\" - Display a Campaign Summary for Management\n",
    "# We use Spark SQL to show the potential impact of this campaign\n",
    "summary_query = \"\"\"\n",
    "    SELECT \n",
    "        CASE \n",
    "            WHEN avg_review_score >= 4.5 THEN 'Promoters (Referral Offer)'\n",
    "            WHEN avg_review_score <= 3.0 THEN 'Detractors (Apology Offer)'\n",
    "            ELSE 'Neutrals (Standard Offer)'\n",
    "        END as Segment,\n",
    "        count(*) as Customer_Count,\n",
    "        sum(monetary_value) as Total_Revenue_At_Risk\n",
    "    FROM olist_hackathon.gold.retention_campaigns\n",
    "    GROUP BY 1\n",
    "    ORDER BY Total_Revenue_At_Risk DESC\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nðŸ“Š Campaign Dashboard:\")\n",
    "display(spark.sql(summary_query))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "05_GenAI_Personalization",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
